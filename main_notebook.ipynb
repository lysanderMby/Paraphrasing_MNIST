{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be run instead of main.py for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class PretrainedClassifier(nn.Module):\n",
    "    \"\"\"Initial classifier trained on original data\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input: 28x28\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # 28x28 -> 28x28\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 28x28 -> 28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
    "\n",
    "        # Size progression:\n",
    "        # After first pool: 14x14x32\n",
    "        # After conv2 + pool: 7x7x64\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv + pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -> 14x14\n",
    "\n",
    "        # Second conv + pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -> 7x7\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ImageParaphraser(nn.Module):\n",
    "    \"\"\"ResNet-based paraphraser that maximally transforms images while preserving classification\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input: 28x28x1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # 28x28x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2),  # 14x14x64 - Use average pooling instead of max pooling\n",
    "\n",
    "            # ResNet-style blocks\n",
    "            self._make_res_block(64, 128),  # 14x14x128\n",
    "            nn.AvgPool2d(2, 2),  # 7x7x128\n",
    "            self._make_res_block(128, 256),  # 7x7x256\n",
    "        )\n",
    "\n",
    "        # Style embedding\n",
    "        self.style_linear = nn.Linear(32, 256)\n",
    "\n",
    "        # Decoder with anti-aliasing\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 7x7x256 -> 14x14x128\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # Use bilinear upsampling\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  # Anti-aliasing convolution\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 14x14x128 -> 28x28x64\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Smooth final output\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _make_res_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create a ResNet-style block with smoothing\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            # Add a small amount of average pooling to suppress high frequencies\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Generate random style vector\n",
    "        style = torch.randn(batch_size, 32).to(x.device)\n",
    "        style = self.style_linear(style)\n",
    "        style = style.view(batch_size, -1, 1, 1)\n",
    "\n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Add style information (reduced magnitude to prevent artifacts)\n",
    "        x = x + 0.05 * style.expand(-1, -1, x.size(2), x.size(3))\n",
    "\n",
    "        # Decode with smoothing\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class InterpretableClassifier(nn.Module):\n",
    "    \"\"\"Classifier that outputs interpretable intermediate representations\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            # Layer 1: Edge detection and basic feature extraction\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "                nn.Sigmoid()\n",
    "            ),\n",
    "            # Layer 2: Pattern recognition\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "                nn.Sigmoid()\n",
    "            ),\n",
    "            # Layer 3: Higher-level feature extraction\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 1, kernel_size=3, padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        intermediates = []\n",
    "        current = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            current = layer(current)\n",
    "            intermediates.append(current)\n",
    "        \n",
    "        logits = self.classifier(current)\n",
    "        \n",
    "        if return_intermediates:\n",
    "            return logits, intermediates\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train initial classifier\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, device, num_epochs=10):\n",
    "    \"\"\"Train the initial classifier on original data\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model = model.to(device)  # Ensure model is on correct device\n",
    "    criterion = criterion.to(device)  # Move loss function to device if needed\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            # Move input data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_acc = evaluate_model(model, test_loader, device)\n",
    "        print(f'Epoch {epoch+1}: Train Acc: {100*correct/total:.2f}%, Test Acc: {test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train paraphraser\n",
    "\n",
    "def train_paraphraser(paraphraser, classifier, train_loader, device, num_epochs=2):\n",
    "    \"\"\"Train the paraphraser with comprehensive logging\"\"\"\n",
    "    # Ensure models are on the correct device\n",
    "    paraphraser = paraphraser.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(paraphraser.parameters(), lr=0.0001)\n",
    "    criterion = ParaphrasingLoss(classifier)  # Assuming ParaphrasingLoss handles device internally\n",
    "\n",
    "    # For logging\n",
    "    epoch_accuracies = []\n",
    "    epoch_losses = []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    print(\"\\nStarting paraphraser training:\")\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        paraphraser.train()\n",
    "        running_loss = 0.0\n",
    "        running_class_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        epoch_confusion = torch.zeros(10, 10)  # Initialize on CPU for accumulation\n",
    "        batch_count = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate paraphrased images\n",
    "            paraphrased = paraphraser(images)\n",
    "\n",
    "            # Compute losses and accuracy\n",
    "            loss, class_loss, accuracy, confusion = criterion(images, paraphrased, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics (move confusion matrix to CPU for accumulation)\n",
    "            running_loss += loss.item()\n",
    "            running_class_loss += class_loss.item()\n",
    "            running_accuracy += accuracy\n",
    "            epoch_confusion += confusion.cpu()\n",
    "            batch_count += 1\n",
    "\n",
    "            # Print progress every 100 batches\n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"Batch {batch_count}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.1f}%\")\n",
    "\n",
    "        # Compute epoch averages\n",
    "        avg_loss = running_loss / batch_count\n",
    "        avg_class_loss = running_class_loss / batch_count\n",
    "        avg_accuracy = running_accuracy / batch_count\n",
    "\n",
    "        # Store metrics\n",
    "        epoch_accuracies.append(avg_accuracy)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        confusion_matrices.append(epoch_confusion)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Classification Loss: {avg_class_loss:.4f}\")\n",
    "        print(f\"Average Classification Accuracy: {avg_accuracy:.1f}%\")\n",
    "\n",
    "        # Display confusion matrix\n",
    "        print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "        confusion_matrix = epoch_confusion.numpy()\n",
    "        row_sums = confusion_matrix.sum(axis=1, keepdims=True)\n",
    "        normalized_confusion = 100 * confusion_matrix / row_sums  # percentage scores\n",
    "\n",
    "        # Print normalized confusion matrix\n",
    "        print(\"\\nNormalized Confusion Matrix (%):\")\n",
    "        print(\"    \", end=\"\")\n",
    "        for i in range(10):\n",
    "            print(f\"{i:5d}\", end=\"\")\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "        for i in range(10):\n",
    "            print(f\"{i:2d} |\", end=\"\")\n",
    "            for j in range(10):\n",
    "                print(f\"{normalized_confusion[i,j]:5.1f}\", end=\"\")  # i is true, j is predicted\n",
    "            print(f\" | {confusion_matrix[i].sum():.0f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        fig = plt.imshow(normalized_confusion, cmap='viridis', interpolation='nearest', origin='lower')\n",
    "        plt.colorbar(fig)\n",
    "        plt.title('Confusion on Paraphrased Images')\n",
    "        plt.xlabel('True Class')\n",
    "        plt.ylabel('Predicted Class')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize examples\n",
    "        visualize_paraphrasing(paraphraser, train_loader, device, num_samples=10)\n",
    "\n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_accuracies)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return epoch_accuracies, epoch_losses, confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train interpretable classifier\n",
    "\n",
    "def train_interpretable_classifier(model, paraphraser, train_loader, test_loader, \n",
    "                                 paraphrase_prob=1.0, num_epochs=5, device='cuda', save_dir=None):\n",
    "    \"\"\"\n",
    "    Train the interpretable classifier. Probabilistically paraphrases input and intermediate model states.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    paraphraser = paraphraser.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    metrics_log = {\n",
    "        'paraphrase_prob': paraphrase_prob,\n",
    "        'train_acc': [],\n",
    "        'train_loss': [],\n",
    "        'test_acc': [],\n",
    "        'batch_metrics': {\n",
    "            'loss': [],\n",
    "            'acc': []\n",
    "        },\n",
    "        'layer_metrics': {i: {'mse': [], 'cosine_sim': [], 'was_paraphrased': []} \n",
    "                         for i in range(len(model.layers) + 1)}  # +1 for input layer\n",
    "    }\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(f\"\\nTraining with paraphrase probability: {paraphrase_prob * 100:.1f}%\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        layer_diffs = [[] for _ in range(len(model.layers) + 1)]  # +1 for input layer\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Handle input paraphrasing\n",
    "            was_input_paraphrased = np.random.random() < paraphrase_prob\n",
    "            if was_input_paraphrased:\n",
    "                with torch.no_grad():\n",
    "                    current = paraphraser(images)\n",
    "                # Calculate metrics between original and paraphrased input\n",
    "                mse = F.mse_loss(images, current).item()\n",
    "                cosine_sim = F.cosine_similarity(images.view(images.size(0), -1),\n",
    "                                               current.view(current.size(0), -1),\n",
    "                                               dim=1).mean().item()\n",
    "            else:\n",
    "                current = images\n",
    "                mse = 0.0  # No paraphrasing occurred\n",
    "                cosine_sim = 1.0  # Identity mapping\n",
    "            \n",
    "            # Store input layer metrics\n",
    "            layer_diffs[0].append({\n",
    "                'mse': mse, \n",
    "                'cosine_sim': cosine_sim,\n",
    "                'was_paraphrased': was_input_paraphrased\n",
    "            })\n",
    "            \n",
    "            # Forward pass tracking intermediates\n",
    "            intermediates = []\n",
    "            paraphrased_intermediates = []\n",
    "            layer_originals = []  # Store original outputs for visualization\n",
    "            \n",
    "            # Process through each layer with probabilistic paraphrasing\n",
    "            for i, layer in enumerate(model.layers, 1):  # Start from 1 since 0 is input\n",
    "                # Get layer output\n",
    "                current = layer(current)\n",
    "                layer_originals.append(current.clone())  # Store original layer output\n",
    "                \n",
    "                # Probabilistically apply paraphrasing\n",
    "                was_paraphrased = np.random.random() < paraphrase_prob\n",
    "                if was_paraphrased:\n",
    "                    with torch.no_grad():\n",
    "                        paraphrased = paraphraser(current)\n",
    "                    # Calculate metrics only if paraphrasing occurred\n",
    "                    mse = F.mse_loss(current, paraphrased).item()\n",
    "                    cosine_sim = F.cosine_similarity(current.view(current.size(0), -1),\n",
    "                                                   paraphrased.view(paraphrased.size(0), -1),\n",
    "                                                   dim=1).mean().item()\n",
    "                else:\n",
    "                    paraphrased = current\n",
    "                    mse = 0.0  # No paraphrasing occurred\n",
    "                    cosine_sim = 1.0  # Identity mapping\n",
    "                \n",
    "                intermediates.append(current)\n",
    "                paraphrased_intermediates.append(paraphrased)\n",
    "                \n",
    "                layer_diffs[i].append({\n",
    "                    'mse': mse, \n",
    "                    'cosine_sim': cosine_sim,\n",
    "                    'was_paraphrased': was_paraphrased\n",
    "                })\n",
    "                \n",
    "                current = paraphrased\n",
    "            \n",
    "            # Final classification\n",
    "            logits = model.classifier(current)\n",
    "            \n",
    "            # Calculate loss\n",
    "            class_loss = criterion(logits, labels)\n",
    "            reg_loss = sum(F.mse_loss(inter, para) \n",
    "                          for inter, para in zip(intermediates, paraphrased_intermediates))\n",
    "            \n",
    "            total_loss = class_loss + 0.1 * reg_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            batch_loss = total_loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            batch_count += 1\n",
    "\n",
    "            # Store batch-level metrics\n",
    "            metrics_log['batch_metrics']['loss'].append(batch_loss)\n",
    "            metrics_log['batch_metrics']['acc'].append(100 * (predicted == labels).sum().item() / labels.size(0))\n",
    "\n",
    "            if batch_idx % 400 == 0:  # Reduced frequency for more meaningful plots\n",
    "                visualize_intermediates(\n",
    "                    original_input=images,\n",
    "                    paraphrased_input=current if was_input_paraphrased else None,\n",
    "                    layer_originals=layer_originals,\n",
    "                    layer_paraphrased=paraphrased_intermediates,\n",
    "                    layer_diffs=layer_diffs,\n",
    "                    epoch=epoch,\n",
    "                    batch_idx=batch_idx\n",
    "                )\n",
    "                \n",
    "                # Log layer statistics\n",
    "                for i in range(len(model.layers) + 1):\n",
    "                    layer_metrics = layer_diffs[i][-100:]  # Use last 100 batches\n",
    "                    metrics_log['layer_metrics'][i]['mse'].append(\n",
    "                        np.mean([m['mse'] for m in layer_metrics]))\n",
    "                    metrics_log['layer_metrics'][i]['cosine_sim'].append(\n",
    "                        np.mean([m['cosine_sim'] for m in layer_metrics]))\n",
    "                    metrics_log['layer_metrics'][i]['was_paraphrased'].append(\n",
    "                        np.mean([m['was_paraphrased'] for m in layer_metrics]))\n",
    "                    \n",
    "                print(f'\\nLayer Statistics (Paraphrase Prob: {paraphrase_prob * 100:.1f}%):')\n",
    "                for i in range(len(model.layers) + 1):\n",
    "                    layer_name = \"Input\" if i == 0 else f\"Layer {i}\"\n",
    "                    paraphrase_rate = np.mean([m['was_paraphrased'] for m in layer_diffs[i][-100:]])\n",
    "                    print(f'{layer_name}:')\n",
    "                    print(f'  MSE: {metrics_log[\"layer_metrics\"][i][\"mse\"][-1]:.4f}')\n",
    "                    print(f'  Cosine Similarity: {metrics_log[\"layer_metrics\"][i][\"cosine_sim\"][-1]:.4f}')\n",
    "                    print(f'  Actual Paraphrase Rate: {paraphrase_rate * 100:.1f}%')\n",
    "\n",
    "        # Epoch-level metrics\n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss = epoch_loss / batch_count\n",
    "        test_acc = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "        metrics_log['train_acc'].append(train_acc)\n",
    "        metrics_log['train_loss'].append(train_loss)\n",
    "        metrics_log['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1} (Paraphrase Prob: {paraphrase_prob * 100:.1f}%):')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        # Plot training progress\n",
    "        plot_training_progress(metrics_log)\n",
    "        \n",
    "        # Save best model if specified\n",
    "        if save_dir and test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            save_path = Path(save_dir) / f'interpretable_classifier_p{paraphrase_prob:.2f}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    return metrics_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphraser loss function\n",
    "\n",
    "class ParaphrasingLoss(nn.Module):\n",
    "    \"\"\"Simplified loss function focused on classification preservation with moderate transformation\"\"\"\n",
    "    def __init__(self, classifier, classification_weight=5.0, transform_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.classification_weight = classification_weight\n",
    "        self.transform_weight = transform_weight\n",
    "\n",
    "    def forward(self, original, paraphrased, true_labels):\n",
    "        # Classification preservation using original classifier\n",
    "        with torch.no_grad():\n",
    "            original_logits = self.classifier(original)\n",
    "        paraphrased_logits = self.classifier(paraphrased)\n",
    "\n",
    "        classification_loss = F.cross_entropy(paraphrased_logits, true_labels)\n",
    "\n",
    "        # Simple L2 difference to encourage moderate transformation\n",
    "        transform_loss = -F.mse_loss(original, paraphrased)  # Negative because we want some difference\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = (self.classification_weight * classification_loss +\n",
    "                     self.transform_weight * transform_loss)\n",
    "\n",
    "        # For logging: compute classification accuracy\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(paraphrased_logits, 1)\n",
    "            correct = (predicted == true_labels).sum().item()\n",
    "            total = true_labels.size(0)\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Get confusion matrix data\n",
    "            confusion = torch.zeros(10, 10, device=original.device)\n",
    "            for t, p in zip(true_labels, predicted):\n",
    "                confusion[t.item(), p.item()] += 1\n",
    "\n",
    "        return total_loss, classification_loss, accuracy, confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation and evaluation\n",
    "\n",
    "def visualize_paraphrasing(paraphraser, train_loader, device, num_samples=10):\n",
    "    \"\"\"Helper function to visualize paraphrased images\"\"\"\n",
    "    # Get a batch of images\n",
    "    images, _ = next(iter(train_loader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    \n",
    "    # Generate paraphrased images\n",
    "    with torch.no_grad():\n",
    "        paraphrased = paraphraser(images)\n",
    "    \n",
    "    # Move tensors to CPU for visualization\n",
    "    original_images = images.cpu()\n",
    "    paraphrased_images = paraphrased.cpu()\n",
    "    \n",
    "    # Plot original vs paraphrased\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(num_samples):\n",
    "        # Original\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(original_images[i].squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Original')\n",
    "        \n",
    "        # Paraphrased\n",
    "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "        plt.imshow(paraphrased_images[i].squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Paraphrased')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "def visualize_intermediates(original_input, paraphrased_input, layer_originals, \n",
    "                          layer_paraphrased, layer_diffs, epoch, batch_idx, num_samples=1):\n",
    "    \"\"\"\n",
    "    Visualize random sample of images, their intermediate representations, and paraphrased versions.\n",
    "    Clearly indicates which versions were actually paraphrased vs copied.\n",
    "    \"\"\"\n",
    "    # Set num_samples to 1 for brevity of output. Including a large number of samples gives better visibility over training\n",
    "    batch_size = original_input.size(0)\n",
    "    indices = torch.randperm(batch_size)[:num_samples]\n",
    "    \n",
    "    for idx in indices:\n",
    "        num_cols = len(layer_originals) + 1  # +1 for input\n",
    "        plt.figure(figsize=(3 * num_cols, 6))\n",
    "        \n",
    "        # Original input\n",
    "        plt.subplot(2, num_cols, 1)\n",
    "        plt.imshow(original_input[idx].cpu().squeeze(), cmap='gray')\n",
    "        plt.title('Original Input')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Input paraphrasing status and visualization\n",
    "        was_paraphrased = layer_diffs[0][-1]['was_paraphrased']\n",
    "        status = \"Paraphrased\" if was_paraphrased else \"Original\"\n",
    "        plt.subplot(2, num_cols, num_cols + 1)\n",
    "        \n",
    "        # Show either paraphrased input or original depending on whether paraphrasing occurred\n",
    "        display_input = paraphrased_input if was_paraphrased else original_input\n",
    "        plt.imshow(display_input[idx].detach().cpu().squeeze(), cmap='gray')\n",
    "        plt.title(f'Input ({status})')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Layer outputs and their paraphrased versions\n",
    "        for i, (orig, para) in enumerate(zip(layer_originals, layer_paraphrased)):\n",
    "            # Original layer output\n",
    "            plt.subplot(2, num_cols, i + 2)\n",
    "            plt.imshow(orig[idx].detach().cpu().squeeze(), cmap='gray')\n",
    "            plt.title(f'Layer {i+1}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Paraphrasing status for this layer\n",
    "            was_paraphrased = layer_diffs[i+1][-1]['was_paraphrased']\n",
    "            status = \"Paraphrased\" if was_paraphrased else \"Original\"\n",
    "            \n",
    "            # Paraphrased or original version\n",
    "            plt.subplot(2, num_cols, num_cols + i + 2)\n",
    "            plt.imshow(para[idx].detach().cpu().squeeze(), cmap='gray')\n",
    "            plt.title(f'L{i+1} ({status})')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Epoch {epoch+1}, Step {batch_idx}, Sample {idx.item()}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def compare_training_results(metrics_logs):\n",
    "    \"\"\"Compare and visualize results from different paraphrasing probabilities\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot test accuracy comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for metrics in metrics_logs:\n",
    "        prob = metrics['paraphrase_prob']\n",
    "        plt.plot(metrics['test_acc'], label=f'Prob={prob:.1f}')\n",
    "    plt.title('Test Accuracy Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot average MSE per layer\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for metrics in metrics_logs:\n",
    "        prob = metrics['paraphrase_prob']\n",
    "        layer_mses = []\n",
    "        for layer in range(len(metrics['layer_metrics'])):\n",
    "            avg_mse = np.mean(metrics['layer_metrics'][layer]['mse'])\n",
    "            layer_mses.append(avg_mse)\n",
    "        plt.plot(layer_mses, marker='o', label=f'Prob={prob:.1f}')\n",
    "    plt.title('Average MSE by Layer')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot average cosine similarity per layer\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for metrics in metrics_logs:\n",
    "        prob = metrics['paraphrase_prob']\n",
    "        layer_cos = []\n",
    "        for layer in range(len(metrics['layer_metrics'])):\n",
    "            avg_cos = np.mean(metrics['layer_metrics'][layer]['cosine_sim'])\n",
    "            layer_cos.append(avg_cos)\n",
    "        plt.plot(layer_cos, marker='o', label=f'Prob={prob:.1f}')\n",
    "    plt.title('Average Cosine Similarity by Layer')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "\n",
    "def plot_training_progress(metrics_log):\n",
    "    \"\"\"Plot detailed training metrics including per-batch information\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(metrics_log['batch_metrics']['loss'], label='Batch Loss', alpha=0.3)\n",
    "    # Add smoothed version\n",
    "    window = min(50, len(metrics_log['batch_metrics']['loss']))\n",
    "    if window > 0:\n",
    "        smoothed = np.convolve(metrics_log['batch_metrics']['loss'], \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smoothed, label='Smoothed Loss', linewidth=2)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(metrics_log['batch_metrics']['acc'], label='Batch Accuracy', alpha=0.3)\n",
    "    # Add smoothed version\n",
    "    if window > 0:\n",
    "        smoothed = np.convolve(metrics_log['batch_metrics']['acc'], \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smoothed, label='Smoothed Accuracy', linewidth=2)\n",
    "    plt.plot(np.arange(0, len(metrics_log['batch_metrics']['acc']), \n",
    "                      len(metrics_log['batch_metrics']['acc'])//len(metrics_log['test_acc'])),\n",
    "             metrics_log['test_acc'], label='Test Accuracy', linewidth=2)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot paraphrasing rates\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i in range(len(metrics_log['layer_metrics'])):\n",
    "        layer_name = \"Input\" if i == 0 else f\"Layer {i}\"\n",
    "        rates = metrics_log['layer_metrics'][i]['was_paraphrased']\n",
    "        plt.plot(rates, label=layer_name)\n",
    "    plt.title('Actual Paraphrasing Rates')\n",
    "    plt.xlabel('Update Step')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "def main():\n",
    "    # Setup device and paths\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    root_dir = Path(os.path.abspath(''))\n",
    "    data_dir = root_dir / 'data'\n",
    "    models_dir = root_dir / 'models'\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Setup data loaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(root=str(data_dir), train=True,\n",
    "                                             download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root=str(data_dir), train=False,\n",
    "                                            download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, \n",
    "                            pin_memory=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, \n",
    "                            pin_memory=True, num_workers=4)\n",
    "    \n",
    "    # Load or train prerequisite models\n",
    "    classifier = PretrainedClassifier()\n",
    "    paraphraser = ImageParaphraser()\n",
    "    \n",
    "    # Load/train classifier\n",
    "    classifier_path = models_dir / 'initial_classifier.pth'\n",
    "    if classifier_path.exists():\n",
    "        print(\"Loading pretrained classifier...\")\n",
    "        classifier.load_state_dict(torch.load(classifier_path, map_location=device))\n",
    "    else:\n",
    "        print(\"Training initial classifier...\")\n",
    "        classifier.to(device)\n",
    "        train_classifier(classifier, train_loader, test_loader, device)\n",
    "        torch.save(classifier.state_dict(), classifier_path)\n",
    "    \n",
    "    classifier.to(device)\n",
    "    classifier.eval()\n",
    "    \n",
    "    # Load/train paraphraser\n",
    "    paraphraser_path = models_dir / 'paraphraser.pth'\n",
    "    if paraphraser_path.exists():\n",
    "        print(\"Loading pretrained paraphraser...\")\n",
    "        paraphraser.load_state_dict(torch.load(paraphraser_path, map_location=device))\n",
    "    else:\n",
    "        print(\"Training paraphraser...\")\n",
    "        paraphraser.to(device)\n",
    "        train_paraphraser(paraphraser, classifier, train_loader, device)\n",
    "        torch.save(paraphraser.state_dict(), paraphraser_path)\n",
    "    \n",
    "    paraphraser.to(device)\n",
    "    paraphraser.eval()\n",
    "    \n",
    "    # Train interpretable classifiers with different paraphrasing probabilities\n",
    "    paraphrase_probs = [0.0, 0.5, 1.0]  # 0%, 50%, 100%\n",
    "    all_metrics = []\n",
    "    \n",
    "    for prob in paraphrase_probs:\n",
    "        print(f\"\\nTraining classifier with {prob * 100:.1f}% paraphrasing probability\")\n",
    "        model = InterpretableClassifier()\n",
    "        metrics = train_interpretable_classifier(\n",
    "            model, \n",
    "            paraphraser,\n",
    "            train_loader, \n",
    "            test_loader, \n",
    "            paraphrase_prob=prob,\n",
    "            device=device,\n",
    "            save_dir=str(models_dir)\n",
    "        )\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        # Save final model and metrics. Saves separate models for each paraphrasing probability given\n",
    "        torch.save(model.state_dict(), \n",
    "                  models_dir / f'interpretable_classifier_p{prob:.2f}.pth')\n",
    "        torch.save(metrics, \n",
    "                  models_dir / f'training_metrics_p{prob:.2f}.pth')\n",
    "    \n",
    "    # Compare results\n",
    "    compare_training_results(all_metrics)\n",
    "    \n",
    "    print(\"\\nTraining complete! Models and metrics saved in:\", models_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
